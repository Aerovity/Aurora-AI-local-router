{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Cactus Models for Router\n",
    "\n",
    "This notebook creates performance profiles for Cactus Compute models to enable intelligent routing.\n",
    "\n",
    "**What this does:**\n",
    "1. Loads benchmark datasets (Alpaca, MMLU, etc.)\n",
    "2. Runs inference on all Cactus models\n",
    "3. Creates clusters based on prompt similarity\n",
    "4. Computes per-cluster error rates for each model\n",
    "5. Saves mobile-optimized router profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import normalize\n",
    "from tqdm import tqdm\n",
    "\n",
    "from core import ProfileConverter, ModelInfo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "CONFIG = {\n",
    "    'n_clusters': 10,  # Smaller for mobile (original uses 20)\n",
    "    'embedding_model': 'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'random_seed': 42,\n",
    "    'max_samples': 500,  # For quick testing, increase for production\n",
    "}\n",
    "\n",
    "# Cactus models to profile\n",
    "CACTUS_MODELS = [\n",
    "    {\n",
    "        'model_id': 'gemma-270m',\n",
    "        'model_path': 'google/gemma-3-270m-it',\n",
    "        'size_mb': 172,\n",
    "        'avg_tokens_per_sec': 173,\n",
    "        'context_size': 2048,\n",
    "        'capabilities': ['text']\n",
    "    },\n",
    "    {\n",
    "        'model_id': 'smollm-360m',\n",
    "        'model_path': 'HuggingFaceTB/SmolLM2-360m-Instruct',\n",
    "        'size_mb': 227,\n",
    "        'avg_tokens_per_sec': 150,\n",
    "        'context_size': 2048,\n",
    "        'capabilities': ['text']\n",
    "    },\n",
    "    {\n",
    "        'model_id': 'qwen-600m',\n",
    "        'model_path': 'Qwen/Qwen3-0.6B',\n",
    "        'size_mb': 394,\n",
    "        'avg_tokens_per_sec': 129,\n",
    "        'context_size': 2048,\n",
    "        'capabilities': ['text', 'tools', 'embed']\n",
    "    },\n",
    "    {\n",
    "        'model_id': 'lfm2-700m',\n",
    "        'model_path': 'LiquidAI/LFM2-700M',\n",
    "        'size_mb': 467,\n",
    "        'avg_tokens_per_sec': 115,\n",
    "        'context_size': 2048,\n",
    "        'capabilities': ['text', 'tools', 'embed']\n",
    "    },\n",
    "    {\n",
    "        'model_id': 'qwen-1.7b',\n",
    "        'model_path': 'Qwen/Qwen3-1.7B',\n",
    "        'size_mb': 1161,\n",
    "        'avg_tokens_per_sec': 75,\n",
    "        'context_size': 2048,\n",
    "        'capabilities': ['text', 'tools', 'embed']\n",
    "    },\n",
    "]\n",
    "\n",
    "print(f\"Profiling {len(CACTUS_MODELS)} models with {CONFIG['n_clusters']} clusters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset\n",
    "\n",
    "For demonstration, we'll create a synthetic dataset. In production, use real benchmarks like Alpaca, MMLU, or your custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option A: Load from file (if you have one)\n",
    "# df = pd.read_csv('../data/benchmark_dataset.csv')\n",
    "\n",
    "# Option B: Create synthetic dataset for demonstration\n",
    "def create_synthetic_dataset(n_samples=500):\n",
    "    \"\"\"Create synthetic QA pairs for demonstration.\"\"\"\n",
    "    prompts = [\n",
    "        # Simple factual questions (easy)\n",
    "        \"What is the capital of France?\",\n",
    "        \"How many continents are there?\",\n",
    "        \"What is 2+2?\",\n",
    "        \"What color is the sky?\",\n",
    "        \"What is water made of?\",\n",
    "        \n",
    "        # Coding questions (medium)\n",
    "        \"Write a Python function to reverse a string\",\n",
    "        \"Explain what a for loop does\",\n",
    "        \"How do you sort a list in Python?\",\n",
    "        \"What is a dictionary in programming?\",\n",
    "        \"Explain recursion with an example\",\n",
    "        \n",
    "        # Complex reasoning (hard)\n",
    "        \"Explain quantum entanglement\",\n",
    "        \"What are the implications of climate change?\",\n",
    "        \"Describe the theory of relativity\",\n",
    "        \"Explain how neural networks work\",\n",
    "        \"What is the meaning of consciousness?\",\n",
    "    ]\n",
    "    \n",
    "    # Repeat and vary prompts\n",
    "    data = []\n",
    "    for i in range(n_samples):\n",
    "        prompt = prompts[i % len(prompts)]\n",
    "        # Add variation\n",
    "        if i % 3 == 0:\n",
    "            prompt = \"Please \" + prompt\n",
    "        elif i % 3 == 1:\n",
    "            prompt = prompt + \" Explain briefly.\"\n",
    "        \n",
    "        data.append({\n",
    "            'input': prompt,\n",
    "            'expected_output': f\"Answer to: {prompt}\",  # Placeholder\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "df = create_synthetic_dataset(CONFIG['max_samples'])\n",
    "print(f\"Dataset: {len(df)} samples\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Extract Embeddings & Create Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load embedding model\n",
    "print(\"Loading embedding model...\")\n",
    "embedding_model = SentenceTransformer(CONFIG['embedding_model'])\n",
    "\n",
    "# Extract embeddings\n",
    "print(\"Extracting embeddings...\")\n",
    "embeddings = embedding_model.encode(\n",
    "    df['input'].tolist(),\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "# Normalize for spherical K-means\n",
    "embeddings_normalized = normalize(embeddings, norm='l2')\n",
    "\n",
    "print(f\"Embeddings shape: {embeddings_normalized.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-means clustering\n",
    "print(f\"Clustering into {CONFIG['n_clusters']} clusters...\")\n",
    "kmeans = KMeans(\n",
    "    n_clusters=CONFIG['n_clusters'],\n",
    "    random_state=CONFIG['random_seed'],\n",
    "    n_init=10,\n",
    "    max_iter=300\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(embeddings_normalized)\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Compute silhouette score\n",
    "silhouette = silhouette_score(embeddings_normalized, cluster_labels)\n",
    "print(f\"Silhouette score: {silhouette:.3f}\")\n",
    "\n",
    "# Show cluster distribution\n",
    "cluster_counts = df['cluster'].value_counts().sort_index()\n",
    "print(\"\\nCluster distribution:\")\n",
    "print(cluster_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simulate Model Performance\n",
    "\n",
    "**Note:** In production, you would run actual inference using Cactus models here.\n",
    "For demonstration, we'll simulate performance based on model size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_model_performance(model_id, cluster_id, model_size_mb):\n",
    "    \"\"\"\n",
    "    Simulate model performance (replace with actual Cactus inference).\n",
    "    \n",
    "    Smaller models have higher error rates, larger models have lower.\n",
    "    \"\"\"\n",
    "    np.random.seed(CONFIG['random_seed'] + cluster_id)\n",
    "    \n",
    "    # Base error rate inversely proportional to model size\n",
    "    # 200MB model: ~15% error, 1000MB model: ~5% error\n",
    "    base_error = max(0.05, 0.20 - (model_size_mb / 1000) * 0.15)\n",
    "    \n",
    "    # Add cluster-specific variation\n",
    "    cluster_variation = np.random.uniform(-0.05, 0.05)\n",
    "    \n",
    "    error_rate = np.clip(base_error + cluster_variation, 0.01, 0.50)\n",
    "    return error_rate\n",
    "\n",
    "# Compute error rates for each model per cluster\n",
    "error_rates = {}\n",
    "\n",
    "for model in CACTUS_MODELS:\n",
    "    model_id = model['model_id']\n",
    "    rates = []\n",
    "    \n",
    "    for cluster_id in range(CONFIG['n_clusters']):\n",
    "        error_rate = simulate_model_performance(\n",
    "            model_id,\n",
    "            cluster_id,\n",
    "            model['size_mb']\n",
    "        )\n",
    "        rates.append(float(error_rate))\n",
    "    \n",
    "    error_rates[model_id] = rates\n",
    "    avg_error = np.mean(rates)\n",
    "    print(f\"{model_id:15s}: {avg_error:.2%} avg error rate\")\n",
    "\n",
    "print(\"\\nError rates computed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Create & Save Router Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create profile using ProfileConverter\n",
    "output_dir = Path('../profiles')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "profile = ProfileConverter.create_cactus_profile(\n",
    "    models_info=CACTUS_MODELS,\n",
    "    error_rates=error_rates,\n",
    "    cluster_centers=kmeans.cluster_centers_,\n",
    "    embedding_model=CONFIG['embedding_model'],\n",
    "    output_path=output_dir / 'cactus_models_profile.json',\n",
    "    lambda_min=0.0,\n",
    "    lambda_max=2.0,\n",
    "    default_cost_preference=0.5\n",
    ")\n",
    "\n",
    "print(\"Profile saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate profile\n",
    "is_valid = ProfileConverter.validate_profile(\n",
    "    output_dir / 'cactus_models_profile.json'\n",
    ")\n",
    "print(f\"Profile valid: {is_valid}\")\n",
    "\n",
    "# Get stats\n",
    "stats = ProfileConverter.get_profile_stats(\n",
    "    output_dir / 'cactus_models_profile.json'\n",
    ")\n",
    "print(\"\\nProfile stats:\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize Clusters (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2, random_state=CONFIG['random_seed'])\n",
    "embeddings_2d = pca.fit_transform(embeddings_normalized)\n",
    "\n",
    "# Plot clusters\n",
    "plt.figure(figsize=(12, 8))\n",
    "scatter = plt.scatter(\n",
    "    embeddings_2d[:, 0],\n",
    "    embeddings_2d[:, 1],\n",
    "    c=cluster_labels,\n",
    "    cmap='tab10',\n",
    "    alpha=0.6,\n",
    "    s=50\n",
    ")\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.title(f'Prompt Clusters (n={CONFIG[\"n_clusters\"]}, silhouette={silhouette:.3f})')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'clusters_visualization.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Visualization saved to {output_dir / 'clusters_visualization.png'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot error rates heatmap\n",
    "import seaborn as sns\n",
    "\n",
    "# Create error rate matrix\n",
    "model_names = [m['model_id'] for m in CACTUS_MODELS]\n",
    "error_matrix = np.array([error_rates[mid] for mid in model_names])\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(\n",
    "    error_matrix,\n",
    "    xticklabels=[f\"C{i}\" for i in range(CONFIG['n_clusters'])],\n",
    "    yticklabels=model_names,\n",
    "    annot=True,\n",
    "    fmt='.2f',\n",
    "    cmap='RdYlGn_r',\n",
    "    vmin=0.0,\n",
    "    vmax=0.3,\n",
    "    cbar_kws={'label': 'Error Rate'}\n",
    ")\n",
    "plt.title('Per-Cluster Error Rates by Model')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Model')\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_dir / 'error_rates_heatmap.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"Heatmap saved to {output_dir / 'error_rates_heatmap.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âœ… Done!\n",
    "\n",
    "You now have:\n",
    "1. `cactus_models_profile.json` - Router profile for Cactus models\n",
    "2. Cluster visualizations\n",
    "3. Error rate heatmaps\n",
    "\n",
    "Next: Use notebook `02_test_routing.ipynb` to test the router!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
